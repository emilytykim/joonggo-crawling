# crawler.py

import os
import time
import csv
import pandas as pd
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from multiprocessing import Pool, cpu_count
from webdriver_manager.chrome import ChromeDriverManager
from tqdm import tqdm


# âœ… ë¡œê·¸ì¸ëœ í¬ë¡¬ ì„¸ì…˜ìœ¼ë¡œ ì‹¤í–‰
def open_logged_in_driver():
    options = Options()
    options.add_argument(
        "user-data-dir=/Users/emily/Library/Application Support/Google/Chrome"
    )
    options.add_argument("profile-directory=Profile 2")
    driver = webdriver.Chrome(options=options)
    return driver


# âœ… ê²Œì‹œê¸€ ëª©ë¡ ìˆ˜ì§‘ (ì œëª©, URL, ë‚ ì§œ, ë‹‰ë„¤ì„)
def get_post_links(driver, category_url, max_pages=5):
    driver.get(category_url)
    time.sleep(2)
    driver.switch_to.frame("cafe_main")

    all_posts = []

    for page in range(1, max_pages + 1):
        print(f"ğŸ“„ Page {page}")
        time.sleep(1)

        html = driver.page_source
        soup = BeautifulSoup(html, "html.parser")
        rows = soup.select("tr[align=center]")

        for row in rows:
            try:
                title_elem = row.select_one("a.article")
                date_elem = row.select_one("td.td_date")
                nick_elem = row.select_one("td.p-nick a.m-tcol-c")

                if not title_elem:
                    continue

                all_posts.append(
                    {
                        "title": title_elem.text.strip(),
                        "url": "https://cafe.naver.com" + title_elem.get("href"),
                        "date": date_elem.text.strip() if date_elem else "",
                        "nickname": nick_elem.text.strip() if nick_elem else "",
                    }
                )
            except:
                continue

        # í˜ì´ì§€ ìˆ«ì ë²„íŠ¼ í´ë¦­
        try:
            next_btn = driver.find_element(
                By.XPATH,
                f"//button[@class='btn number' and text()='{(page % 10) or 10}']",
            )
            next_btn.click()
        except:
            print("â›”ï¸ ë” ì´ìƒ í˜ì´ì§€ ì—†ìŒ")
            break

        # 10í˜ì´ì§€ ë‹¨ìœ„ í™”ì‚´í‘œ í´ë¦­
        if page % 10 == 0:
            try:
                arrow_btn = driver.find_element(
                    By.CSS_SELECTOR, ".Pagination .btn.type_next"
                )
                arrow_btn.click()
            except:
                break

    return all_posts


# âœ… ìƒì„¸ í˜ì´ì§€ ìˆ˜ì§‘ (ë³‘ë ¬)
def get_post_details(post):
    try:
        options = Options()
        options.add_argument(
            "user-data-dir=/Users/emily/Library/Application Support/Google/Chrome"
        )
        options.add_argument("profile-directory=Profile 2")
        options.add_argument("--headless")
        options.add_argument("--disable-gpu")
        driver = webdriver.Chrome(options=options)

        driver.get(post["url"])
        time.sleep(1)

        html = driver.page_source
        soup = BeautifulSoup(html, "html.parser")

        price_elem = soup.select_one("strong.cost")
        post["price"] = price_elem.text.strip() if price_elem else "N/A"

        status_elem = soup.select_one("em.SaleLabel")
        post["status"] = status_elem.text.strip() if status_elem else "íŒë§¤ì¤‘"

        driver.quit()
    except Exception as e:
        print(f"âŒ ìƒì„¸í˜ì´ì§€ ì˜¤ë¥˜: {post['url']}, {e}")
        post["price"] = "N/A"
        post["status"] = "N/A"

    return post


# âœ… ì „ì²´ ì¹´í…Œê³ ë¦¬ ì‹¤í–‰
def crawl_category(category_name, category_url, output_csv, max_pages=5):
    print(f"ğŸš€ Start: {category_name}")
    driver = open_logged_in_driver()
    try:
        posts = get_post_links(driver, category_url, max_pages)
    finally:
        driver.quit()

    print(f"ğŸ” ìƒì„¸í˜ì´ì§€ ë³‘ë ¬ íŒŒì‹± ì¤‘... (ì´ {len(posts)}ê°œ)")
    with Pool(processes=cpu_count()) as pool:
        detailed_posts = pool.map(get_post_details, posts)

    os.makedirs(os.path.dirname(output_csv), exist_ok=True)
    with open(output_csv, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(
            f, fieldnames=["title", "url", "date", "nickname", "price", "status"]
        )
        writer.writeheader()
        writer.writerows(detailed_posts)

    print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_csv} ({len(detailed_posts)} rows)")


class JoonggoCrawler:
    def __init__(self, headless=False):
        self.setup_driver(headless)
        
    def setup_driver(self, headless):
        """Chrome WebDriver ì„¤ì •"""
        chrome_options = Options()
        if headless:
            chrome_options.add_argument('--headless=new')
        
        # ê¸°ë³¸ ì˜µì…˜ ì„¤ì •
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--disable-extensions')
        chrome_options.add_argument('--disable-popup-blocking')
        
        # ì„ì‹œ í”„ë¡œí•„ ë””ë ‰í† ë¦¬ ì‚¬ìš©
        import tempfile
        import os
        temp_dir = os.path.join(tempfile.gettempdir(), 'chrome_profile')
        os.makedirs(temp_dir, exist_ok=True)
        chrome_options.add_argument(f'--user-data-dir={temp_dir}')
        
        # ì¶”ê°€ ì„¤ì •
        chrome_options.add_experimental_option('excludeSwitches', ['enable-automation'])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=chrome_options)
        self.wait = WebDriverWait(self.driver, 10)
        
    def switch_to_iframe(self):
        """iframeìœ¼ë¡œ ì „í™˜"""
        try:
            iframe = self.wait.until(
                EC.presence_of_element_located((By.ID, "cafe_main"))
            )
            self.driver.switch_to.frame(iframe)
            return True
        except Exception as e:
            print(f"iframe ì „í™˜ ì‹¤íŒ¨: {e}")
            return False
            
    def get_post_urls(self, category_url, max_pages=1):
        """ê²Œì‹œê¸€ URL ìˆ˜ì§‘"""
        self.driver.get(category_url)
        time.sleep(2)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        
        if not self.switch_to_iframe():
            return []
            
        post_urls = []
        for page in range(1, max_pages + 1):
            try:
                # ê²Œì‹œê¸€ ëª©ë¡ì—ì„œ URL ì¶”ì¶œ (ê³µì§€ì‚¬í•­ ì œì™¸)
                articles = self.wait.until(
                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, "tr:not(.board-notice) a.article"))
                )
                
                for article in articles:
                    url = article.get_attribute('href')
                    if url:
                        post_urls.append(url)
                        
                # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
                if page < max_pages:
                    next_btn = self.driver.find_element(By.CSS_SELECTOR, "a.pgR")
                    next_btn.click()
                    time.sleep(1)
                    
            except Exception as e:
                print(f"í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                break
                
        return post_urls
        
    def extract_post_data(self, url):
        """ê²Œì‹œê¸€ ìƒì„¸ ì •ë³´ ì¶”ì¶œ"""
        try:
            self.driver.get(url)
            time.sleep(2)
            
            if not self.switch_to_iframe():
                return None
                
            # ì œëª© ì¶”ì¶œ
            title = self.wait.until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "h3.title_text"))
            ).text
            
            # ê°€ê²© ì¶”ì¶œ
            try:
                price = self.driver.find_element(By.CSS_SELECTOR, "div.ProductPrice strong.cost").text
            except:
                price = "ê°€ê²© ì •ë³´ ì—†ìŒ"
                
            # íŒë§¤ ìƒíƒœ ì¶”ì¶œ
            try:
                status = self.driver.find_element(By.CSS_SELECTOR, "em.SaleLabel").text
            except:
                status = "íŒë§¤ì¤‘"
                
            # ë‹‰ë„¤ì„ ì¶”ì¶œ
            try:
                nickname = self.driver.find_element(By.CSS_SELECTOR, "div.nick_box button.nickname").text
            except:
                nickname = "ë‹‰ë„¤ì„ ì—†ìŒ"
                
            # ë‚ ì§œ ì¶”ì¶œ
            try:
                date = self.driver.find_element(By.CSS_SELECTOR, "div.article_info span.date").text
            except:
                date = "ë‚ ì§œ ì •ë³´ ì—†ìŒ"
                
            # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ (URLì—ì„œ ì¶”ì¶œ)
            category = "ì—¬ì„±íŒ¨ì…˜"  # ê¸°ë³¸ê°’
            
            return {
                'category': category,
                'title': title,
                'price': price,
                'status': status,
                'date': date,
                'nickname': nickname,
                'url': url
            }
            
        except Exception as e:
            print(f"ê²Œì‹œê¸€ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨ ({url}): {e}")
            return None
            
    def crawl_category(self, category_url, max_pages=1):
        """ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì‹¤í–‰"""
        print(f"ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì‹œì‘: {category_url}")
        
        # ê²Œì‹œê¸€ URL ìˆ˜ì§‘
        post_urls = self.get_post_urls(category_url, max_pages)
        print(f"ìˆ˜ì§‘ëœ ê²Œì‹œê¸€ URL ìˆ˜: {len(post_urls)}")
        
        # ê²Œì‹œê¸€ ë°ì´í„° ìˆ˜ì§‘
        posts_data = []
        for url in tqdm(post_urls, desc="ê²Œì‹œê¸€ ë°ì´í„° ìˆ˜ì§‘ ì¤‘"):
            post_data = self.extract_post_data(url)
            if post_data:
                posts_data.append(post_data)
                
        # DataFrame ìƒì„± ë° CSV ì €ì¥
        if posts_data:
            df = pd.DataFrame(posts_data)
            output_file = f"joonggo_data_{int(time.time())}.csv"
            df.to_csv(output_file, index=False, encoding='utf-8-sig')
            print(f"ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_file}")
            
        return posts_data
        
    def close(self):
        """WebDriver ì¢…ë£Œ"""
        self.driver.quit()
