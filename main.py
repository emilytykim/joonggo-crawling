from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
import csv
import os

from webdriver_manager.chrome import ChromeDriverManager
from crawler import JoonggoCrawler


def setup_driver():
    options = Options()
    # options.add_argument("--headless")  # ì°½ ì•ˆ ë„ìš°ë ¤ë©´
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920x1080")
    options.add_argument("user-agent=Mozilla/5.0")

    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    return driver


def switch_to_iframe(driver):
    WebDriverWait(driver, 10).until(
        EC.frame_to_be_available_and_switch_to_it((By.ID, "cafe_main"))
    )


def extract_posts(driver, page_num):
    base_url = f"https://cafe.naver.com/ArticleList.nhn?search.clubid=10050146&search.menuid=356&search.boardtype=L&search.page={page_num}"
    driver.get(base_url)
    switch_to_iframe(driver)

    try:
        rows = driver.find_elements(By.CSS_SELECTOR, "table.board-list tbody tr")
        post_urls = []
        for row in rows:
            try:
                link = row.find_element(By.CSS_SELECTOR, "a.article")
                href = link.get_attribute("href")
                if href:
                    full_url = "https://cafe.naver.com" + href
                    post_urls.append(full_url)
            except:
                continue
        return post_urls
    except Exception as e:
        print(f"âŒ Failed to extract post URLs: {e}")
        return []


def extract_detail(driver, url):
    driver.get(url)
    switch_to_iframe(driver)

    try:
        title = driver.find_element(By.CSS_SELECTOR, "h3.title_text").text.strip()
    except:
        title = ""

    try:
        price = driver.find_element(
            By.CSS_SELECTOR, "div.ProductPrice strong.cost"
        ).text.strip()
    except:
        price = ""

    try:
        content = driver.find_element(
            By.CSS_SELECTOR, "div.se-main-container"
        ).text.strip()
    except:
        content = ""

    return {"url": url, "title": title, "price": price, "content": content}


def crawl_category(category_name="ì—¬ì„±íŒ¨ì…˜", pages=3):
    driver = setup_driver()
    time.sleep(10)
    results = []

    for page in range(1, pages + 1):
        print(f"ğŸ“„ Crawling page {page}")
        post_urls = extract_posts(driver, page)

        for post_url in post_urls:
            detail = extract_detail(driver, post_url)
            if detail["title"]:
                results.append(detail)
                print(f"âœ… {detail['title']}")

        time.sleep(1)

    driver.quit()
    save_csv(results, category_name)


def save_csv(data, category_name):
    os.makedirs("results", exist_ok=True)
    path = f"results/{category_name}.csv"
    with open(path, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(f, fieldnames=["url", "title", "price", "content"])
        writer.writeheader()
        writer.writerows(data)
    print(f"âœ… {category_name} â†’ ì €ì¥ ì™„ë£Œ: {path} ({len(data)}ê±´)")


def main():
    # ì—¬ì„±íŒ¨ì…˜ ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ URL
    category_url = "https://cafe.naver.com/joonggonara/ArticleList.nhn?search.clubid=10050146&search.menuid=356&search.boardtype=L"
    
    # ë§ˆì§€ë§‰ìœ¼ë¡œ í¬ë¡¤ë§í•œ URL (Noneì´ë©´ ì²˜ìŒë¶€í„° í¬ë¡¤ë§)
    last_url = None  # ì—¬ê¸°ì— ë§ˆì§€ë§‰ìœ¼ë¡œ í¬ë¡¤ë§í•œ URLì„ ë„£ìœ¼ë©´ ë©ë‹ˆë‹¤
    
    # í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (headless=Falseë¡œ ì„¤ì •í•˜ì—¬ ë¸Œë¼ìš°ì € í‘œì‹œ)
    crawler = JoonggoCrawler(headless=False)
    
    try:
        # ë„¤ì´ë²„ ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ì´ë™
        crawler.driver.get("https://nid.naver.com/nidlogin.login")
        print("ë„¤ì´ë²„ ë¡œê·¸ì¸ì„ ì§„í–‰í•´ì£¼ì„¸ìš”...")
        input("ë¡œê·¸ì¸ì´ ì™„ë£Œë˜ë©´ ì—”í„°ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”...")
        
        # ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 2í˜ì´ì§€ë§Œ í¬ë¡¤ë§)
        crawler.crawl_category(category_url, max_pages=2, last_url=last_url)
        
    except Exception as e:
        print(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
    finally:
        # ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸°
        input("í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì¢…ë£Œí•˜ë ¤ë©´ ì—”í„°ë¥¼ ëˆ„ë¥´ì„¸ìš”...")
        crawler.close()


if __name__ == "__main__":
    main()
